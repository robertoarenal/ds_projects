{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "033462b2-fb8f-4c44-a199-ab086a44558a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from pickle import dump\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ded610fc-fb0f-4c15-91a9-7898501b551a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>business</td>\n",
       "      <td>Ad sales boost Time Warner profit  Quarterly p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>business</td>\n",
       "      <td>Dollar gains on Greenspan speech  The dollar h...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>business</td>\n",
       "      <td>Yukos unit buyer faces loan claim  The owners ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>business</td>\n",
       "      <td>High fuel prices hit BA's profits  British Air...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>business</td>\n",
       "      <td>Pernod takeover talk lifts Domecq  Shares in U...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   category                                            article\n",
       "0  business  Ad sales boost Time Warner profit  Quarterly p...\n",
       "1  business  Dollar gains on Greenspan speech  The dollar h...\n",
       "2  business  Yukos unit buyer faces loan claim  The owners ...\n",
       "3  business  High fuel prices hit BA's profits  British Air...\n",
       "4  business  Pernod takeover talk lifts Domecq  Shares in U..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from utils_articles import TopicTrain\n",
    "articles_df = TopicTrain()._loadArticles('data')\n",
    "articles_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0749c156-d5ce-4151-bf3a-4d9d36412153",
   "metadata": {},
   "outputs": [],
   "source": [
    "def spacy_tokenizer(doc):\n",
    "        \"\"\"Function that serves as tokenizer in our pipeline\n",
    "        Loads the 'en_core_web_sm' model, tokenize the string and perform pre processing. \n",
    "        Preprocessing includes lemmatizing tokens as well as removing stop words and punctuations. \n",
    "        Args:\n",
    "            doc(str): sentence to tokenize.\n",
    "        Returns: \n",
    "            list: preprocessed tokens. \n",
    "        \"\"\"\n",
    "\n",
    "        punctuations = string.punctuation\n",
    "        \n",
    "        stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
    "        tokens = nlp(doc)\n",
    "\n",
    "        # Lemmatizing each token and converting each token into lowercase\n",
    "        tokens = [word.lemma_.lower() for word in tokens if not word.is_space]        \n",
    "        # Removing stop words and punctuations\n",
    "        tokens = [ word for word in tokens if word not in stop_words and word not in punctuations ]\n",
    "        # return preprocessed list of tokens\n",
    "        return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b98d8bf-5e2b-4fc6-9504-34f5a4b0b305",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm')\n",
    "text_clf = Pipeline([('tfidf', TfidfVectorizer(tokenizer=spacy_tokenizer,min_df=3)),\\\n",
    "             ('clf', RandomForestClassifier())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "303f48a1-536c-46a3-8347-b4a0c96ba3e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/nlp_app/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>Pipeline(steps=[(&#x27;tfidf&#x27;,\n",
       "                 TfidfVectorizer(min_df=3,\n",
       "                                 tokenizer=&lt;function spacy_tokenizer at 0x7f84aa073400&gt;)),\n",
       "                (&#x27;clf&#x27;, RandomForestClassifier())])</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">Pipeline</label><div class=\"sk-toggleable__content\"><pre>Pipeline(steps=[(&#x27;tfidf&#x27;,\n",
       "                 TfidfVectorizer(min_df=3,\n",
       "                                 tokenizer=&lt;function spacy_tokenizer at 0x7f84aa073400&gt;)),\n",
       "                (&#x27;clf&#x27;, RandomForestClassifier())])</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">TfidfVectorizer</label><div class=\"sk-toggleable__content\"><pre>TfidfVectorizer(min_df=3,\n",
       "                tokenizer=&lt;function spacy_tokenizer at 0x7f84aa073400&gt;)</pre></div></div></div><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier()</pre></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "Pipeline(steps=[('tfidf',\n",
       "                 TfidfVectorizer(min_df=3,\n",
       "                                 tokenizer=<function spacy_tokenizer at 0x7f84aa073400>)),\n",
       "                ('clf', RandomForestClassifier())])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_clf.fit(articles_df['article'], articles_df['category'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b4c21acf-da4a-4c24-873f-587507ad518d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/src/nlp/articles/model/rm_tfidf.pkl\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "model_path = os.path.join(str(pathlib.Path().absolute()), \"model\")\n",
    "model_file = model_path + \"/rm_tfidf.pkl\"\n",
    "print(model_file)\n",
    "dump(text_clf, open(model_file, 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fceb9050-d3c7-467f-bebc-49654ccb51e0",
   "metadata": {},
   "source": [
    "# Loading and predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aabc69e2-78dd-42cc-8937-41b7731e58c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pathlib\n",
    "import joblib\n",
    "\n",
    "model_path = os.path.join(str(pathlib.Path().absolute()), \"model\")\n",
    "model_file = model_path + \"/rm_tfidf.pkl\"\n",
    "#print(model_file)\n",
    "model = joblib.load(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2fe5cc9a-0a97-4b88-8b25-cd35c41c9b1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "def _loadArticles(path):\n",
    "        cat_article = []\n",
    "        for subdir, dirs, files in os.walk(path):\n",
    "            #print(subdir,dirs,files)\n",
    "            for file in files:\n",
    "                if '.txt' in file:\n",
    "                    category = subdir.split('/')[-1]\n",
    "                    f = open(os.path.join(subdir, file),'r')\n",
    "                    lines = f.readlines()\n",
    "                    lines = ' '.join(lines).replace('\\n','')\n",
    "                    #list of lists: [category,article]\n",
    "                    cat_article.append([category,lines])\n",
    "                    f.close()\n",
    "        data = pd.DataFrame(cat_article)\n",
    "        data.columns = ['category','article']\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "27c54293-b2fc-4af4-a7f7-dc226a34be27",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>article</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_articles</td>\n",
       "      <td>Barclays boss Jes Staley is \"shell-shocked, an...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_articles</td>\n",
       "      <td>For nearly seven months, through the regular s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_articles</td>\n",
       "      <td>Tennessee Titans running back Derrick Henry su...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test_articles</td>\n",
       "      <td>There will still be a Facebook, and an Instagr...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        category                                            article\n",
       "0  test_articles  Barclays boss Jes Staley is \"shell-shocked, an...\n",
       "1  test_articles  For nearly seven months, through the regular s...\n",
       "2  test_articles  Tennessee Titans running back Derrick Henry su...\n",
       "3  test_articles  There will still be a Facebook, and an Instagr..."
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_art = _loadArticles('test_articles')\n",
    "target_art.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c65c8c6f-8787-4512-8975-bbca3d552f7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(\"For nearly seven months, through the regular season and the early rounds of the playoffs, the Houston Astros featured the best offense in baseball. It was the highest-scoring, best-balanced, most unsolvable attack in the majors.  The Astros hit for power. They hit for average. They walked the fine line between discipline and aggression: They were the best two-strike hitting team in the big leagues. In an era when so much of the battle is decided by the team that wins the strike zone, the Astros' batsmen generally won the strike zone.\",\n",
       "      dtype='<U539')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "inp = target_art['article'].values[1]\n",
    "inp_arr = np.array(inp)\n",
    "inp_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4afe2c45-7c86-4abe-9d3b-ef8fe2f1c8de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sport']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.06, 0.18, 0.05, 0.71, 0.  ])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = model.predict([inp])\n",
    "proba = model.predict_proba([inp])\n",
    "print(pred)\n",
    "proba[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1b37e826-e0bf-468f-81be-ca64db46da6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/nlp_app/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from utils_articles import TopicTrain\n",
    "TopicTrain().train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "714b6c23-e079-44fe-9691-e18ffbfb9633",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
